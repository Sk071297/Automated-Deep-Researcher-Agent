{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sk071297/automated-deep-researcher-agent-capstone-project?scriptVersionId=279280049\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ü§ñ Automated Deep Researcher Agent (Gemini + SerpApi + CrewAI)\n\nThis notebook builds a **multi-agent deep research system** using:\n\n- **Gemini** (via `GOOGLE_API_KEY`) as the main LLM  \n- **SerpApi** (via `SERPAPI_API_KEY`) for web search  \n- **CrewAI** for coordinating multiple agents  \n- **crewai-tools** for search + web scraping  \n\nThe pipeline:\n\n1. **Search Agent** ‚Üí Finds high-quality sources using SerpApi  \n2. **Scraper Agent** ‚Üí Scrapes the main content from each URL  \n3. **Analyzer Agent** ‚Üí Extracts key insights and themes  \n4. **Writer Agent** ‚Üí Produces a well-structured research report in Markdown  \n\nWe‚Äôll go step-by-step, just like a clean Kaggle notebook. üöÄ","metadata":{}},{"cell_type":"markdown","source":"# üü¶ Step 0 ‚Äì Create a new Kaggle Notebook\n\nGo to https://www.kaggle.com/\n\nClick ‚ÄúCode‚Äù ‚Üí ‚ÄúNew Notebook‚Äù.\n\nIt will open a blank notebook with one empty code cell.","metadata":{}},{"cell_type":"markdown","source":"# üü¶ Step 1 ‚Äì Add your API keys as Kaggle Secrets\n\nBefore writing code, set up secrets:\n\nIn the right sidebar, click ‚ÄúSettings‚Äù.\n\nScroll to ‚ÄúSecrets‚Äù section.\n\nAdd two secrets:\n\nName: GOOGLE_API_KEY ‚Üí Value: your Gemini API key\n\nName: SERPAPI_API_KEY ‚Üí Value: your SerpApi API key\n\nMake sure they are saved (you should see them listed).\n\nNow we‚Äôre ready to code.","metadata":{}},{"cell_type":"markdown","source":"# üü¶Step 2 ‚Äì Install required packages (Cell 1)\n\nIn the first code cell, paste this and run:","metadata":{}},{"cell_type":"code","source":"import sys\n\n!{sys.executable} -m pip install --upgrade \\\n    crewai \\\n    crewai-tools \\\n    langchain-google-genai \\\n    serpapi \\\n    beautifulsoup4 \\\n    requests","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After it finishes:\n\n(Optional but recommended) click ‚ÄúRuntime ‚Üí Restart session‚Äù.\n\nThen continue with the next steps.","metadata":{}},{"cell_type":"markdown","source":"# üü¶ Step 3 ‚Äì Load API keys (Cell 2)\n\nAdd a new code cell and paste:","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    secrets = UserSecretsClient()\n    \n    # Load Google Gemini key\n    GOOGLE_API_KEY = secrets.get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"‚úÖ Gemini API key setup complete.\")\n    \n    # Load SERPAPI key\n    SERPAPI_API_KEY = secrets.get_secret(\"SERPAPI_API_KEY\")\n    os.environ[\"SERPAPI_API_KEY\"] = SERPAPI_API_KEY   # ‚úÖ FIXED\n    print(\"‚úÖ SERPAPI_API_KEY setup complete.\")\n\nexcept Exception as e:\n    print(\n        f\"üîë Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' and 'SERPAPI_API_KEY' to Kaggle secrets. Details: {e}\"\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üü¶ Step 4 ‚Äì Configure Gemini LLM + tools and Define the four agents (Cell 3)\n\nAdd a new code cell and paste:","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nfrom crewai import Agent, LLM\nfrom crewai_tools import SerpApiGoogleSearchTool, ScrapeWebsiteTool\n\n# --- Secrets ---\nsecrets = UserSecretsClient()\nos.environ[\"GOOGLE_API_KEY\"] = secrets.get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"SERPAPI_API_KEY\"] = secrets.get_secret(\"SERPAPI_API_KEY\")\n\n# --- LLM (Gemini) ---\nllm = LLM(model=\"gemini-2.5-flash\")\n\n# --- Tools ---\nsearch_tool = SerpApiGoogleSearchTool()   # SerpApi search\nwebsite_scraper_tool = ScrapeWebsiteTool()  # üëà HERE\nprint(\"‚úÖ search_tool and website_scraper_tool ready.\")\n\n# --- Agents ---\n\nsearch_agent = Agent(\n    role=\"Web Search Strategist\",\n    goal=\"Accurately identify and retrieve the most relevant web pages for a given research query.\",\n    backstory=(\n        \"You are an expert at crafting precise search queries and sifting through search results \"\n        \"to pinpoint valuable, authoritative sources. You prioritize diverse and recent information.\"\n    ),\n    llm=llm,\n    tools=[search_tool],\n    verbose=True,\n    allow_delegation=False,\n)\n\nscraper_agent = Agent(\n    role=\"Information Extractor\",\n    goal=\"Efficiently scrape and extract the main textual content from provided URLs.\",\n    backstory=(\n        \"You are skilled at navigating web pages and extracting their core content, ignoring boilerplate, \"\n        \"advertisements, and irrelevant elements. You handle various website structures.\"\n    ),\n    llm=llm,\n    tools=[website_scraper_tool],  # ‚úÖ now defined\n    verbose=True,\n    allow_delegation=False,\n)\n\nanalyzer_agent = Agent(\n    role=\"Data Analyst & Synthesizer\",\n    goal=(\n        \"Thoroughly read, analyze, and synthesize extracted content to identify key insights, arguments, \"\n        \"and supporting data points relevant to the research query. Identify main viewpoints and potential biases.\"\n    ),\n    backstory=(\n        \"You possess an exceptional ability to discern patterns, extract critical information, identify contradictions \"\n        \"or biases within diverse texts, and connect disparate pieces of information into a cohesive understanding.\"\n    ),\n    llm=llm,\n    verbose=True,\n    allow_delegation=True,\n)\n\nwriter_agent = Agent(\n    role=\"Report Composer\",\n    goal=(\n        \"Compile all analyzed insights into a comprehensive, well-structured, and easy-to-understand research report, \"\n        \"addressing the initial query fully.\"\n    ),\n    backstory=(\n        \"You are a master storyteller and academic writer, capable of weaving complex information into a clear, \"\n        \"concise, and compelling narrative. You ensure logical flow, proper formatting, and accurate representation \"\n        \"of all findings.\"\n    ),\n    llm=llm,\n    verbose=True,\n    allow_delegation=False,\n)\n\nprint(\"‚úÖ Agents created.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 5","metadata":{}},{"cell_type":"code","source":"from google.api_core.exceptions import ResourceExhausted","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nfrom google.genai import Client\nfrom google.api_core.exceptions import ResourceExhausted, GoogleAPIError\n\nclient = Client(api_key=\"GOOGLE_API_KEY\")\n\ndef safe_generate(prompt):\n    retries = 5\n    for attempt in range(retries):\n        try:\n            return client.models.generate(\n                model=\"gemini-2.5-flash\",\n                prompt=prompt\n            )\n        except ResourceExhausted:\n            wait = (attempt + 1) * 5\n            print(f\"Quota limit hit. Retrying in {wait}s...\")\n            time.sleep(wait)\n\n        except GoogleAPIError as e:\n            print(\"Unexpected API error:\", e)\n            break\n\n    raise Exception(\"Failed after retries\")\n\nprint(\"Running Successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üü¶ Step 5 ‚Äì Define the tasks (Cell 4)\n\nAdd a new code cell and paste:","metadata":{}},{"cell_type":"code","source":"from crewai import Task, Crew\n\n# üß™ You‚Äôll pass the actual query later via crew.kickoff(inputs=...)\n# e.g. research_query = \"latest trends in agentic AI\"\n\n# Task for Search Agent\nsearch_task = Task(\n    description=(\n        'You are the Web Search Strategist. Find 5‚Äì10 highly relevant and authoritative web pages or '\n        'articles for the research query: \"{query}\". Prioritize diverse sources like academic papers, '\n        'reputable news outlets, and expert blogs.'\n    ),\n    expected_output=(\n        'A markdown-formatted list of URLs (max 10) that are most likely to contain in-depth information '\n        'on the research query. Format:\\n'\n        '- [Title](URL) ‚Äì 1‚Äì2 bullet points summarizing why this source is useful.'\n    ),\n    agent=search_agent,\n)\n\n# Task for Scraper Agent\nscrape_task = Task(\n    description=(\n        'You are the Information Extractor. For each URL provided in the search results, scrape the main '\n        'content and return a list of cleaned texts. Focus on article body / main content and ignore ads, '\n        'navigation, and unrelated boilerplate.'\n    ),\n    expected_output=(\n        'A markdown-formatted list where each item corresponds to one URL:\\n'\n        '- URL: <url>\\n'\n        '  - Content: \"\"\"<cleaned main text>\"\"\"\\n'\n        'If a URL fails to scrape, include a note like: \"Failed to scrape: <reason>\".'\n    ),\n    agent=scraper_agent,\n    context=[search_task],  # Uses output from search_task\n)\n\n# Task for Analyzer Agent\nanalyze_task = Task(\n    description=(\n        'You are the Data Analyst & Synthesizer. Read all the scraped content and extract key arguments, '\n        'facts, statistics, and recurring themes relevant to the initial query: \"{query}\". Identify the main '\n        'viewpoints, supporting evidence, and any conflicting information. Group similar ideas logically.'\n    ),\n    expected_output=(\n        'A structured summary of key insights, arguments, and data points, clearly categorized into sections '\n        'such as \"Main Themes\", \"Key Arguments\", \"Important Data & Statistics\", \"Conflicting Viewpoints\", '\n        'and \"Notable Biases or Limitations\". Ensure it directly addresses the original research query.'\n    ),\n    agent=analyzer_agent,\n    context=[scrape_task],  # Uses output from scrape_task\n)\n\n# Task for Writer Agent\nwrite_report_task = Task(\n    description=(\n        'You are the Report Composer. Using the analyzed insights, compile a comprehensive, well-structured '\n        'research report. The report should have:\\n'\n        '1. Introduction ‚Äì set the context and state the purpose of the report for \"{query}\".\\n'\n        '2. Main Body ‚Äì sections for each key theme or viewpoint identified by the Analyzer Agent.\\n'\n        '3. Conclusion ‚Äì summarize the findings and offer final thoughts.\\n'\n        'Ensure the report flows logically, is easy to read, and fully addresses the original query: \"{query}\".'\n    ),\n    expected_output=(\n        'A final, well-formatted research report in markdown format.\\n'\n        'Title: \"{query} ‚Äì Research Report\".\\n'\n        'Use headings (##, ###) and bullet points where helpful.'\n    ),\n    agent=writer_agent,\n    context=[analyze_task],  # Uses output from analyze_task\n)\n\nprint(\"‚úÖ Tasks created and wired to agents.\")\n\n# (Optional) put everything into a Crew and run it:\ncrew = Crew(\n    agents=[search_agent, scraper_agent, analyzer_agent, writer_agent],\n    tasks=[search_task, scrape_task, analyze_task, write_report_task],\n    verbose=True,\n)\n\n# Example run:\n# research_query = \"latest trends in agentic AI\"\n# result = crew.kickoff(inputs={\"query\": research_query})\n# print(result)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üü¶ Step 7 ‚Äì Create the Crew, run it, and save the report (Cell 6)\n\nAdd one more code cell and paste:","metadata":{}},{"cell_type":"code","source":"# ===========================================\n#  Deep Researcher Multi-Agent (Gemini + SerpApi)\n#  Complete Program\n# ===========================================\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\nfrom crewai import Agent, Task, Crew, Process, LLM\nfrom crewai_tools import SerpApiGoogleSearchTool, ScrapeWebsiteTool\n\n# ------------------------------\n# 1. Load API keys from Kaggle\n# ------------------------------\nsecrets = UserSecretsClient()\n\n# Google Gemini key\nGOOGLE_API_KEY = secrets.get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\nprint(\"‚úÖ GOOGLE_API_KEY (Gemini) setup complete.\")\n\n# SerpApi key\nSERPAPI_API_KEY = secrets.get_secret(\"SERPAPI_API_KEY\")\nos.environ[\"SERPAPI_API_KEY\"] = SERPAPI_API_KEY\nprint(\"‚úÖ SERPAPI_API_KEY (SerpApi) setup complete.\")\n\n# ------------------------------\n# 2. Configure LLM (Gemini via CrewAI)\n# ------------------------------\nllm = LLM(model=\"gemini-2.5-flash\")\nprint(\"‚úÖ Gemini LLM configured.\")\n\n# ------------------------------\n# 3. Tools: SerpApi search + scraper\n# ------------------------------\nsearch_tool = SerpApiGoogleSearchTool()   # Uses SERPAPI_API_KEY\nwebsite_scraper_tool = ScrapeWebsiteTool()\n\nprint(\"‚úÖ search_tool and website_scraper_tool ready.\")\n\n# ------------------------------\n# 4. Agents\n# ------------------------------\n\n# Research Agent\nsearch_agent = Agent(\n    role=\"Web Search Strategist\",\n    goal=\"Accurately identify and retrieve the most relevant web pages for a given research query.\",\n    backstory=(\n        \"You are an expert at crafting precise search queries and sifting through search results \"\n        \"to pinpoint valuable, authoritative sources. You prioritize diverse and recent information.\"\n    ),\n    llm=llm,\n    tools=[search_tool],\n    verbose=True,\n    allow_delegation=False,\n)\n\n# Scraper Agent\nscraper_agent = Agent(\n    role=\"Information Extractor\",\n    goal=\"Efficiently scrape and extract the main textual content from provided URLs.\",\n    backstory=(\n        \"You are skilled at navigating web pages and extracting their core content, ignoring boilerplate, \"\n        \"advertisements, and irrelevant elements. You handle various website structures.\"\n    ),\n    llm=llm,\n    tools=[website_scraper_tool],\n    verbose=True,\n    allow_delegation=False,\n)\n\n# Analyzer Agent\nanalyzer_agent = Agent(\n    role=\"Data Analyst & Synthesizer\",\n    goal=(\n        \"Thoroughly read, analyze, and synthesize extracted content to identify key insights, arguments, \"\n        \"and supporting data points relevant to the research query. Identify main viewpoints and potential biases.\"\n    ),\n    backstory=(\n        \"You possess an exceptional ability to discern patterns, extract critical information, identify contradictions \"\n        \"or biases within diverse texts, and connect disparate pieces of information into a cohesive understanding.\"\n    ),\n    llm=llm,\n    verbose=True,\n    allow_delegation=True,\n)\n\n# Writer Agent\nwriter_agent = Agent(\n    role=\"Report Composer\",\n    goal=(\n        \"Compile all analyzed insights into a comprehensive, well-structured, and easy-to-understand research report, \"\n        \"addressing the initial query fully.\"\n    ),\n    backstory=(\n        \"You are a master storyteller and academic writer, capable of weaving complex information into a clear, \"\n        \"concise, and compelling narrative. You ensure logical flow, proper formatting, and accurate representation \"\n        \"of all findings.\"\n    ),\n    llm=llm,\n    verbose=True,\n    allow_delegation=False,\n)\n\nprint(\"‚úÖ Agents created.\")\n\n# ------------------------------\n# 5. Tasks\n# ------------------------------\n\n# Search Task\nsearch_task = Task(\n    description=(\n        'You are the Web Search Strategist. Find 5‚Äì10 highly relevant and authoritative web pages or '\n        'articles for the research query: \"{query}\". Prioritize diverse sources like academic papers, '\n        'reputable news outlets, and expert blogs.'\n    ),\n    expected_output=(\n        'A markdown-formatted list of URLs (max 10) that are most likely to contain in-depth information '\n        'on the research query. Format:\\n'\n        '- [Title](URL) ‚Äì 1‚Äì2 bullet points summarizing why this source is useful.'\n    ),\n    agent=search_agent,\n)\n\n# Scrape Task\nscrape_task = Task(\n    description=(\n        'You are the Information Extractor. For each URL provided in the search results, scrape the main '\n        'content and return a list of cleaned texts. Focus on article body / main content and ignore ads, '\n        'navigation, and unrelated boilerplate.'\n    ),\n    expected_output=(\n        'A markdown-formatted list where each item corresponds to one URL:\\n'\n        '- URL: <url>\\n'\n        '  - Content: \"\"\"<cleaned main text>\"\"\"\\n'\n        'If a URL fails to scrape, include a note like: \"Failed to scrape: <reason>\".'\n    ),\n    agent=scraper_agent,\n    context=[search_task],\n)\n\n# Analyze Task\nanalyze_task = Task(\n    description=(\n        'You are the Data Analyst & Synthesizer. Read all the scraped content and extract key arguments, '\n        'facts, statistics, and recurring themes relevant to the initial query: \"{query}\". Identify the main '\n        'viewpoints, supporting evidence, and any conflicting information. Group similar ideas logically.'\n    ),\n    expected_output=(\n        'A structured summary of key insights, arguments, and data points, clearly categorized into sections '\n        'such as \"Main Themes\", \"Key Arguments\", \"Important Data & Statistics\", \"Conflicting Viewpoints\", '\n        'and \"Notable Biases or Limitations\". Ensure it directly addresses the original research query.'\n    ),\n    agent=analyzer_agent,\n    context=[scrape_task],\n)\n\n# Write Report Task\nwrite_report_task = Task(\n    description=(\n        'You are the Report Composer. Using the analyzed insights, compile a comprehensive, well-structured '\n        'research report. The report should have:\\n'\n        '1. Introduction ‚Äì set the context and state the purpose of the report for \"{query}\".\\n'\n        '2. Main Body ‚Äì sections for each key theme or viewpoint identified by the Analyzer Agent.\\n'\n        '3. Conclusion ‚Äì summarize the findings and offer final thoughts.\\n'\n        'Ensure the report flows logically, is easy to read, and fully addresses the original query: \"{query}\".'\n    ),\n    expected_output=(\n        'A final, well-formatted research report in markdown format.\\n'\n        'Title: \"{query} ‚Äì Research Report\".\\n'\n        'Use headings (##, ###) and bullet points where helpful.'\n    ),\n    agent=writer_agent,\n    context=[analyze_task],\n)\n\nprint(\"‚úÖ Tasks created.\")\n\n# ------------------------------\n# 6. Crew\n# ------------------------------\nresearch_crew = Crew(\n    agents=[search_agent, scraper_agent, analyzer_agent, writer_agent],\n    tasks=[search_task, scrape_task, analyze_task, write_report_task],\n    verbose=True,\n    process=Process.sequential,   # tasks run in order\n)\n\nprint(\"########################\")\nprint(\"## Deep-Researcher Agent Initialized (Gemini) ##\")\nprint(\"########################\\n\")\n\n# ------------------------------\n# 7. Run the crew\n# ------------------------------\nresearch_query = input(\"Enter your research query: \")\n\nresult = research_crew.kickoff(inputs={\"query\": research_query})\n\n# Extract final text safely from CrewOutput\nif hasattr(result, \"output_text\"):\n    final_text = result.output_text\nelif hasattr(result, \"final_output\"):\n    final_text = result.final_output\nelse:\n    final_text = str(result)\n\nprint(\"\\n\\n########################\")\nprint(\"### FINAL RESEARCH REPORT ###\")\nprint(\"########################\\n\")\nprint(final_text)\n\n# ------------------------------\n# 8. Save report to file\n# ------------------------------\nsafe_name = (\n    research_query.replace(\" \", \"_\")\n    .replace(\"/\", \"_\")\n    .replace(\":\", \"_\")\n    .replace(\"?\", \"\")\n)[:50]\n\noutput_filename = f\"{safe_name}_report_gemini.md\"\n\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    f.write(final_text)\n\nprint(f\"\\nüìÅ Report saved to {output_filename}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}